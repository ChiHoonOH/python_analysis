{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "\n",
    "- 신경망의 결합\n",
    "- 입력층, 출력층 사이에 중간층(은폐층)을 구성\n",
    "- 중간층에 합성공층과 풀링층을 배치\n",
    "> 이미지 인식, 이미지 블럭 처리(흐리게)  \n",
    "> 경계선을 강조 \n",
    "> 해상도를 낮추는 작업\n",
    "- 구조\n",
    "[입력층] => [중간층:[합성곱층][풀링층] [합성곱층][풀링층] [합성곱층][풀링층] ..... [전결합층]]=>[출력층] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 합성곱층 \n",
    "\n",
    "- 이미지 특징을 추출할 때 사용\n",
    "- 방식  \n",
    "> 입력 x 의 이미지의 일부분을 잘라가면서 가중치 필터(W:평활화 작업, 윤곽선 검출)를 적용하여 맵(C)를 추출하는 과정  \n",
    "> 평활화 : 명암의 분포를 균일하게 처리  \n",
    "> 윤곽선 검출 : 에지 디틱션, 이미지의 윤곽만을 추출해내는 과정  \n",
    "<img src='./data/dp1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀링층\n",
    "\n",
    "- 합성곱층의 연산의 결과로 얻은 특정맵(C)를 축소하는 층    \n",
    "- 특성을 유지 한상태로 축소, 위치 변경에 결과 변화를 막아준다.  \n",
    "- 직선 인식이 미세하게 흐트러져도 직선으로 인식하게 한다.\n",
    "- 방법 : 최대 풀링, 최소 풀링 존재\n",
    "<img src='./data/dp2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전결합층\n",
    "\n",
    "- 각 층의 유닛을 결합  \n",
    "- 합성곱층과 풀링층의 결과 2차원 특징맵을 1차원으로 전개하는 역할을 담당."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST 손글씨 이미지를 딥러닝으로 구현하여 인식  \n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-882d23b65040>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('./data/mnist/', one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] (55000, 784)\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]] (55000, 10)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "print(mnist.train.images, mnist.train.images.shape)\n",
    "print(mnist.train.labels, mnist.train.labels.shape) # label을 이진화 하는게 성능이 좋음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.where(mnist.train.labels[:1][0] > 0)[0][0]\n",
    "for label in mnist.train.labels:\n",
    "    print(type(label.nonzero()[0]))\n",
    "    print(label.nonzero()[0][0])\n",
    "    break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 총 픽셀의 크기 \n",
    "pixels = mnist.train.images.shape[1]\n",
    "# 총 레이블의 특성수 (정답 종류의 수)\n",
    "nums = mnist.train.labels.shape[1]\n",
    "pixels, nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개별 가로,세로 픽셀의 크기는 정사각형이다 -> 제곱\n",
    "pixel_size = int(np.sqrt(pixels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서플로우 절차 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력층,출력층 placeholder 구성\n",
    "\n",
    "x = tf.placeholder(tf.float32,shape=(None, pixels), name='x')\n",
    "\n",
    "y_ = tf.placeholder(tf.float32,shape=(None, nums), name='y_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치(w), 바이어스(b)를 초기화하는 함수  ??뭐가 또나옴?? 아마 [합성곱층][풀링층]이 여러개라서 그런듯\n",
    "\n",
    "def makeWeight(name, filt): # weight => 필터 \n",
    "    # 절단정규분포(truncate normal distribution) 로 부터 난수를 반환받는다.\n",
    "    # 내부적으로 생성된 값들 평균과 표준편차를 가진 정규분포\n",
    "    # 2개를 생성해서 평균보다 떨어지는 값을 버리고, 재선택 되는 과정을 반복한다. \n",
    "    # stddev는 표준편차 값.\n",
    "    wei_init = tf.truncated_normal(filt,stddev=0.1) # ?? 이부분 이해안감. 꼭 이렇게 해야한다는 것이 아니라, 하나의 가정, 예시 \n",
    "    W = tf.Variable(wei_init, name='W_'+name)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeBias(name,size):\n",
    "    bia_init =tf.constant(0.1, shape=[size])\n",
    "    b = tf.Variable(bia_init, name='b_'+name)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 합성곱 계층을 만드는 함수 구성\n",
    "def conv2d(x, W):\n",
    "    # x=> [batch, height, width, channels] # 크기를 모를때 None 말고도 -1 도 가능하다.\n",
    "    # W : [filter_h, filter_w, in_channels, out_channels]\n",
    "    # strides : 크기가 4인 1차원 리스트\n",
    "    # 0 , 3번째는 무조건 1, ?? 이건 왜 그런가?\n",
    "    # 1,2 번째는 이동에 관련 크기 지정.\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')# padding 나가는거 크기? stride는 그림에 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 풀링층 생성 함수\n",
    "# 4*4 => 2*2 ->strides 2 \n",
    "def max_pool(x):\n",
    "    # x=> [batch, height, width, channels] => ReLU를 통과한 값 #합성공에서 POOLING으로넘어 올때 CHANNEL 주의\n",
    "    # ksize :  자리수 4개, 입력 데이터의 각 차원의 윈도우 크기(우리는 2*2) #뭔말 님아??\n",
    "    # 2칸씩 이동하여 출력 결과 1을 만들어 내는 의미\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],\n",
    "                          strides=[1,2,2,1], \n",
    "                          padding='SAME') # SAME이라고는 하지만 실제로는 크기가 줄어듬(정사각형태 유지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 합성곱층 1번 구성\n",
    "NAME1='conv1'\n",
    "with tf.name_scope(NAME1) as scope:\n",
    "    # 가중치 준비\n",
    "    # [5,5,1,32] : 5x5크기의 1개 채널이 들어가서 32개 \n",
    "    # 채널로 나오는 구성\n",
    "    W_conv1 = makeWeight(NAME1,[5,5,1,32]) # output이 자꾸 늘어난다는 말의 의미를 모르겟음. 64 1028\n",
    "    # 바이어스 몇개? 32개(wight의 32개 채널과 동일)\n",
    "    b_conv1 = makeBias(NAME1, 32)    \n",
    "    # 합성곱 계층을 만드는 함수\n",
    "    # x => [~784~] => [batch, heigh,width, channels]\n",
    "    x_img = tf.reshape(x,[-1, pixel_size, pixel_size, 1]) # 여기서 channel 값이 1인게 현재    \n",
    "    h_conv1 = tf.nn.relu(conv2d(x_img, W_conv1)+b_conv1) # 역할 : 값이 0 이하이면 0, 그이상이면 그대로 세팅 함수, bias로 인해서 음수가 나올 수도 있음.\n",
    "    #성능이 좋아서 씀\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 풀링층 생성\n",
    "with tf.name_scope('pool1') as scope:\n",
    "    h_pool1 = max_pool(h_conv1) # 풀링을 통해 반으로 줄어듬. ?? 위에 그림에서 필터를 쓰면 차원이 줄어 드는것 처럼 보였는데.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(14), Dimension(14), Dimension(32)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_pool1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 합성곱층 2번 구성\n",
    "NAME2='conv2'\n",
    "with tf.name_scope(NAME2) as scope:\n",
    "    W_conv2 = makeWeight(NAME1,[5,5,32,64]) #왜 곱하기 2가 됫나??     \n",
    "    b_conv2 = makeBias(NAME1, 64)            \n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2)+b_conv2) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두번째 풀링층\n",
    "with tf.name_scope('pool2') as scope:\n",
    "    h_pool2 = max_pool(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(7), Dimension(7), Dimension(64)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_pool2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전 결합층 구성\n",
    "# 픽셀의 현재 크기는 28/2/2 => 7이다.\n",
    "NAME3 = 'fully_connected'\n",
    "with tf.name_scope(NAME3) as scope:\n",
    "    n = 7 * 7 * 64\n",
    "    # 전 결합층은 최종 입력대비 몇개의 출력이 나올 것인지 정하면 됨\n",
    "    W_fc = makeWeight(NAME3,[n,1024]) #왜 곱하기 2가 됫나??     \n",
    "    b_fc = makeBias(NAME3, 1024)            \n",
    "    \n",
    "    h_pool2_reshape = tf.reshape(h_pool2,[-1, n])\n",
    "    h_fc = tf.nn.relu(tf.matmul(h_pool2_reshape,W_fc)+b_fc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(1024)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_fc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력층\n",
    "# 드롭아웃(과잉 적합) 처리\n",
    "with tf.name_scope('dropout') as scope:\n",
    "    # 과잉 적합 처리를 위해 float형 placeholder 배치\n",
    "    prob = tf.placeholder(tf.float32)\n",
    "    h_fc_drop = tf.nn.dropout(h_fc,prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력 진행 => 소프트맥스\n",
    "NAME4 = 'out'\n",
    "with tf.name_scope(NAME4) as scope:\n",
    "    # 최종 결과물은 h_fc_drop => 1024 => 10으로\n",
    "    W_out = makeWeight(NAME4,[1024,10]) #왜 곱하기 2가 됫나??     \n",
    "    b_out = makeBias(NAME4, 10)  \n",
    "    y_out=tf.nn.softmax(tf.matmul(h_fc_drop, W_out)+b_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "with tf.name_scope('loss') as scope:\n",
    "    # cross entropy\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "with tf.name_scope('training') as scope:\n",
    "    # 확률적 경사 하강법 ; 무작위로 초기화한 매개변수를 손실 함수 최소화\n",
    "    Optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "    train = Optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가\n",
    "with tf.name_scope('predict') as scope:\n",
    "    # 예측\n",
    "    predict = tf.equal(tf.argmax(y_out,1),tf.argmax(y_,1))\n",
    "    # 정확도\n",
    "    accuracy = tf.reduce_mean(tf.cast(predict,tf.float32))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed_dict용 함수\n",
    "def make_feed(img, label, prob1):\n",
    "    # 이미지 데이터, 레이블(특성) 데이터, prob:(평가:1 / 훈련 0.5 ) ??? 뭔말임? 평가일때 1 을 넣으라는 말이다.\n",
    "    return {x:img, y_:label, prob:prob1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 accuracy: Tensor(\"predict/Mean:0\", shape=(), dtype=float32) loss: 539.8864\n",
      "step: 10 accuracy: Tensor(\"predict/Mean:0\", shape=(), dtype=float32) loss: 259.38525\n",
      "step: 20 accuracy: Tensor(\"predict/Mean:0\", shape=(), dtype=float32) loss: 246.81769\n",
      "step: 30 accuracy: Tensor(\"predict/Mean:0\", shape=(), dtype=float32) loss: 191.8215\n",
      "step: 40 accuracy: Tensor(\"predict/Mean:0\", shape=(), dtype=float32) loss: 98.25636\n",
      "step: 50 accuracy: Tensor(\"predict/Mean:0\", shape=(), dtype=float32) loss: 128.51935\n",
      "step: 60 accuracy: Tensor(\"predict/Mean:0\", shape=(), dtype=float32) loss: 89.919876\n",
      "step: 70 accuracy: Tensor(\"predict/Mean:0\", shape=(), dtype=float32) loss: 81.05855\n",
      "step: 80 accuracy: Tensor(\"predict/Mean:0\", shape=(), dtype=float32) loss: 79.21171\n",
      "step: 90 accuracy: Tensor(\"predict/Mean:0\", shape=(), dtype=float32) loss: 58.88674\n",
      "최종 loss= 59.70147 acc= 0.8305\n"
     ]
    }
   ],
   "source": [
    "# 세션 시작\n",
    "with tf.Session() as sess:\n",
    "    # 1. tensorflow 변수 초기화\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # 2. 텐서보드 준비\n",
    "    tf.summary.FileWriter('./data/log_cnn', graph=sess.graph)\n",
    "    # 3. 테스트용 전용 피드 ??피드가 뭐냐?\n",
    "    test_feed = make_feed(mnist.test.images, mnist.test.labels, 1)\n",
    "    # 훈련의 횟수는 총 데이터를 기준으로 산정\n",
    "    # 여기서는 시간 관계상 100번만 수행 \n",
    "    for step in range(100):\n",
    "        # 데이터 50개씩만 사용\n",
    "        batch = mnist.train.next_batch(50) \n",
    "        # print(batch) # images, labels \n",
    "        # print(batch[0].shape, batch[1].shape)\n",
    "        # break\n",
    "        # 훈련데이터 피드구성\n",
    "        train_feed = make_feed(batch[0], batch[1], 0.5)\n",
    "        \n",
    "        # 훈련\n",
    "        t, loss = sess.run([train, cross_entropy],feed_dict=train_feed) \n",
    "        # 중간점검\n",
    "        if step % 10 == 0:\n",
    "            acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "            print('step:',step,'accuracy:',accuracy,'loss:',loss)\n",
    "    \n",
    "    # 최종결과\n",
    "    \n",
    "    acc= sess.run(accuracy, feed_dict=test_feed)\n",
    "    print('최종 loss=', loss, 'acc=', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
